{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio del módulo 6 de Ciencia de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: diamonds\n",
    "\n",
    "* PARTE 1 (10 %) Carga de datos de diamonds desde CSV con schema: https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/diamonds.csv\n",
    "\n",
    "* PARTE 2 (40 %) Pipeline regresión price con preprocesados\n",
    "  * Imputer, StringIndexer, OneHotEncoder, MinMaxScaler o StandardScaler, VectorAssembler\n",
    "\n",
    "* PARTE 3 (40 %) Pipeline clasificación multiclase sobre variable cut con preprocesados\n",
    "  * Imputer, StringIndexer, OneHotEncoder, MinMaxScaler o StandardScaler, VectorAssembler\n",
    "\n",
    "* PARTE 4 (10 %) Gridsearch con CrossValidation sobre cualquiera de los pipelines\n",
    "\n",
    "Los modelos, se puede utilizar RandomForest para los dos por ejemplo o el que se quiera. Ejemplo RandomForestRegressor para regresión y MultiLayerPerceptronClassifier para clasificación.\n",
    "\n",
    "m6_nombre_apellido.ipynb\n",
    "\n",
    "Usar pyspark MLlib y dataframes de pyspark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Busco actualizaciones de Ubuntu:\n",
    "\n",
    "wsl --update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probar docker\n",
    "\n",
    "Primero abrir la aplicación Docker Desktop.\n",
    "\n",
    "docker run hello-world\n",
    "\n",
    "Si todo va bien sale esto:\n",
    "\n",
    "Hello from Docker!\n",
    "This message shows that your installation appears to be working correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "escribimos en la consola de cmd o de ubuntu:\n",
    "\n",
    "docker run -it --name pyspark -p 8888:8888 jupyter/pyspark-notebook\n",
    "\n",
    "se abrirá el proceso, nos dará uina dirección que hay que abrir en el navegador, y poner en el Kernel de Visual Studio Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, IntegerType, NumericType\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, Imputer, OneHotEncoder, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor, GBTRegressor, LinearRegression\n",
    "from pyspark.sql.functions import col, sum \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e27ce936af63:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pipeline_diamonds</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9017acea50>"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('pipeline_diamonds').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llamamos el dataset, que está en esta url:\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/diamonds.csv'\n",
    "csv_path = 'diamonds.csv'\n",
    "with open(csv_path, 'wb') as file:\n",
    "    file.write(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pedimos el esquema del archivo:   \n",
    "schema = StructType([\n",
    "    StructField('carat', FloatType(), True),\n",
    "    StructField('cut', StringType(), True),\n",
    "    StructField('color', StringType(), True),\n",
    "    StructField('clarity', StringType(), True),\n",
    "    StructField('depth', FloatType(), True),\n",
    "    StructField('table', FloatType(), True),\n",
    "    StructField('price', IntegerType(), True),\n",
    "    StructField('x', FloatType(), True),\n",
    "    StructField('y', FloatType(), True),\n",
    "    StructField('z', FloatType(), True),\n",
    "])\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=False, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'price', 'x', 'y', 'z']"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+---------+-----+-------+------------------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|summary|             carat|      cut|color|clarity|             depth|             table|            price|                 x|                 y|                 z|\n",
      "+-------+------------------+---------+-----+-------+------------------+------------------+-----------------+------------------+------------------+------------------+\n",
      "|  count|             53940|    53940|53940|  53940|             53940|             53940|            53940|             53940|             53940|             53940|\n",
      "|   mean|0.7979397459442544|     NULL| NULL|   NULL|61.749404890324215|57.457183908399585|3932.799721913237| 5.731157212872659| 5.734525955793015|3.5387337920972493|\n",
      "| stddev| 0.474011242836904|     NULL| NULL|   NULL| 1.432621320665403|2.2344905638396657|3989.439738146397|1.1217607437465076|1.1421346736743894| 0.705698843275196|\n",
      "|    min|               0.2|     Fair|    D|     I1|              43.0|              43.0|              326|               0.0|               0.0|               0.0|\n",
      "|    25%|               0.4|     NULL| NULL|   NULL|              61.0|              56.0|              950|              4.71|              4.72|              2.91|\n",
      "|    50%|               0.7|     NULL| NULL|   NULL|              61.8|              57.0|             2401|               5.7|              5.71|              3.53|\n",
      "|    75%|              1.04|     NULL| NULL|   NULL|              62.5|              59.0|             5324|              6.54|              6.54|              4.04|\n",
      "|    max|              5.01|Very Good|    J|   VVS2|              79.0|              95.0|            18823|             10.74|              58.9|              31.8|\n",
      "+-------+------------------+---------+-----+-------+------------------+------------------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# equivalente al describe de pandas\n",
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|carat|    cut|color|clarity|depth|table|price|   x|   y|   z|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "| 0.23|  Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n",
      "| 0.21|Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n",
      "| 0.23|   Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n",
      "| 0.29|Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|\n",
      "| 0.31|   Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5) # Equivale al head de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- carat: float (nullable = true)\n",
      " |-- cut: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- clarity: string (nullable = true)\n",
      " |-- depth: float (nullable = true)\n",
      " |-- table: float (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- x: float (nullable = true)\n",
      " |-- y: float (nullable = true)\n",
      " |-- z: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vemos el esquema:\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('carat', 'float'),\n",
       " ('cut', 'string'),\n",
       " ('color', 'string'),\n",
       " ('clarity', 'string'),\n",
       " ('depth', 'float'),\n",
       " ('table', 'float'),\n",
       " ('price', 'int'),\n",
       " ('x', 'float'),\n",
       " ('y', 'float'),\n",
       " ('z', 'float')]"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(df.withColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|      cut|count|\n",
      "+---------+-----+\n",
      "|  Premium|13791|\n",
      "|    Ideal|21551|\n",
      "|     Good| 4906|\n",
      "|     Fair| 1610|\n",
      "|Very Good|12082|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# agrupar datos\n",
    "# equivalente a value_counts de pandas\n",
    "df.groupBy('cut').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|color|count|\n",
      "+-----+-----+\n",
      "|    F| 9542|\n",
      "|    E| 9797|\n",
      "|    D| 6775|\n",
      "|    J| 2808|\n",
      "|    G|11292|\n",
      "|    I| 5422|\n",
      "|    H| 8304|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('color').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|clarity|count|\n",
      "+-------+-----+\n",
      "|   VVS2| 5066|\n",
      "|    SI1|13065|\n",
      "|     IF| 1790|\n",
      "|     I1|  741|\n",
      "|   VVS1| 3655|\n",
      "|    VS2|12258|\n",
      "|    SI2| 9194|\n",
      "|    VS1| 8171|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('clarity').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESIÓN PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LinearRegression in module pyspark.ml.regression:\n",
      "\n",
      "class LinearRegression(_JavaRegressor, _LinearRegressionParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n",
      " |  LinearRegression(*, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, regParam: float = 0.0, elasticNetParam: float = 0.0, tol: float = 1e-06, fitIntercept: bool = True, standardization: bool = True, solver: str = 'auto', weightCol: Optional[str] = None, aggregationDepth: int = 2, loss: str = 'squaredError', epsilon: float = 1.35, maxBlockSizeInMB: float = 0.0)\n",
      " |  \n",
      " |  Linear regression.\n",
      " |  \n",
      " |  The learning objective is to minimize the specified loss function, with regularization.\n",
      " |  This supports two kinds of loss:\n",
      " |  \n",
      " |  * squaredError (a.k.a squared loss)\n",
      " |  * huber (a hybrid of squared error for relatively small errors and absolute error for     relatively large ones, and we estimate the scale parameter from training data)\n",
      " |  \n",
      " |  This supports multiple types of regularization:\n",
      " |  \n",
      " |  * none (a.k.a. ordinary least squares)\n",
      " |  * L2 (ridge regression)\n",
      " |  * L1 (Lasso)\n",
      " |  * L2 + L1 (elastic net)\n",
      " |  \n",
      " |  .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  Fitting with huber loss only supports none and L2 regularization.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from pyspark.ml.linalg import Vectors\n",
      " |  >>> df = spark.createDataFrame([\n",
      " |  ...     (1.0, 2.0, Vectors.dense(1.0)),\n",
      " |  ...     (0.0, 2.0, Vectors.sparse(1, [], []))], [\"label\", \"weight\", \"features\"])\n",
      " |  >>> lr = LinearRegression(regParam=0.0, solver=\"normal\", weightCol=\"weight\")\n",
      " |  >>> lr.setMaxIter(5)\n",
      " |  LinearRegression...\n",
      " |  >>> lr.getMaxIter()\n",
      " |  5\n",
      " |  >>> lr.setRegParam(0.1)\n",
      " |  LinearRegression...\n",
      " |  >>> lr.getRegParam()\n",
      " |  0.1\n",
      " |  >>> lr.setRegParam(0.0)\n",
      " |  LinearRegression...\n",
      " |  >>> model = lr.fit(df)\n",
      " |  >>> model.setFeaturesCol(\"features\")\n",
      " |  LinearRegressionModel...\n",
      " |  >>> model.setPredictionCol(\"newPrediction\")\n",
      " |  LinearRegressionModel...\n",
      " |  >>> model.getMaxIter()\n",
      " |  5\n",
      " |  >>> model.getMaxBlockSizeInMB()\n",
      " |  0.0\n",
      " |  >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n",
      " |  >>> abs(model.predict(test0.head().features) - (-1.0)) < 0.001\n",
      " |  True\n",
      " |  >>> abs(model.transform(test0).head().newPrediction - (-1.0)) < 0.001\n",
      " |  True\n",
      " |  >>> abs(model.coefficients[0] - 1.0) < 0.001\n",
      " |  True\n",
      " |  >>> abs(model.intercept - 0.0) < 0.001\n",
      " |  True\n",
      " |  >>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n",
      " |  >>> abs(model.transform(test1).head().newPrediction - 1.0) < 0.001\n",
      " |  True\n",
      " |  >>> lr.setParams(featuresCol=\"vector\")\n",
      " |  LinearRegression...\n",
      " |  >>> lr_path = temp_path + \"/lr\"\n",
      " |  >>> lr.save(lr_path)\n",
      " |  >>> lr2 = LinearRegression.load(lr_path)\n",
      " |  >>> lr2.getMaxIter()\n",
      " |  5\n",
      " |  >>> model_path = temp_path + \"/lr_model\"\n",
      " |  >>> model.save(model_path)\n",
      " |  >>> model2 = LinearRegressionModel.load(model_path)\n",
      " |  >>> model.coefficients[0] == model2.coefficients[0]\n",
      " |  True\n",
      " |  >>> model.intercept == model2.intercept\n",
      " |  True\n",
      " |  >>> model.transform(test0).take(1) == model2.transform(test0).take(1)\n",
      " |  True\n",
      " |  >>> model.numFeatures\n",
      " |  1\n",
      " |  >>> model.write().format(\"pmml\").save(model_path + \"_2\")\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearRegression\n",
      " |      _JavaRegressor\n",
      " |      Regressor\n",
      " |      pyspark.ml.wrapper.JavaPredictor\n",
      " |      pyspark.ml.base.Predictor\n",
      " |      pyspark.ml.wrapper.JavaEstimator\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      pyspark.ml.base.Estimator\n",
      " |      _LinearRegressionParams\n",
      " |      pyspark.ml.base._PredictorParams\n",
      " |      pyspark.ml.param.shared.HasLabelCol\n",
      " |      pyspark.ml.param.shared.HasFeaturesCol\n",
      " |      pyspark.ml.param.shared.HasPredictionCol\n",
      " |      pyspark.ml.param.shared.HasRegParam\n",
      " |      pyspark.ml.param.shared.HasElasticNetParam\n",
      " |      pyspark.ml.param.shared.HasMaxIter\n",
      " |      pyspark.ml.param.shared.HasTol\n",
      " |      pyspark.ml.param.shared.HasFitIntercept\n",
      " |      pyspark.ml.param.shared.HasStandardization\n",
      " |      pyspark.ml.param.shared.HasWeightCol\n",
      " |      pyspark.ml.param.shared.HasSolver\n",
      " |      pyspark.ml.param.shared.HasAggregationDepth\n",
      " |      pyspark.ml.param.shared.HasLoss\n",
      " |      pyspark.ml.param.shared.HasMaxBlockSizeInMB\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, regParam: float = 0.0, elasticNetParam: float = 0.0, tol: float = 1e-06, fitIntercept: bool = True, standardization: bool = True, solver: str = 'auto', weightCol: Optional[str] = None, aggregationDepth: int = 2, loss: str = 'squaredError', epsilon: float = 1.35, maxBlockSizeInMB: float = 0.0)\n",
      " |      __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True,                  standardization=True, solver=\"auto\", weightCol=None, aggregationDepth=2,                  loss=\"squaredError\", epsilon=1.35, maxBlockSizeInMB=0.0)\n",
      " |  \n",
      " |  setAggregationDepth(self, value: int) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`aggregationDepth`.\n",
      " |  \n",
      " |  setElasticNetParam(self, value: float) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`elasticNetParam`.\n",
      " |  \n",
      " |  setEpsilon(self, value: float) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`epsilon`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  setFitIntercept(self, value: bool) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`fitIntercept`.\n",
      " |  \n",
      " |  setLoss(self, value: str) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`loss`.\n",
      " |  \n",
      " |  setMaxBlockSizeInMB(self, value: float) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`maxBlockSizeInMB`.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |  \n",
      " |  setMaxIter(self, value: int) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`maxIter`.\n",
      " |  \n",
      " |  setParams(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, regParam: float = 0.0, elasticNetParam: float = 0.0, tol: float = 1e-06, fitIntercept: bool = True, standardization: bool = True, solver: str = 'auto', weightCol: Optional[str] = None, aggregationDepth: int = 2, loss: str = 'squaredError', epsilon: float = 1.35, maxBlockSizeInMB: float = 0.0) -> 'LinearRegression'\n",
      " |      setParams(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                   maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True,                   standardization=True, solver=\"auto\", weightCol=None, aggregationDepth=2,                   loss=\"squaredError\", epsilon=1.35, maxBlockSizeInMB=0.0)\n",
      " |      Sets params for linear regression.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setRegParam(self, value: float) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`regParam`.\n",
      " |  \n",
      " |  setSolver(self, value: str) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`solver`.\n",
      " |  \n",
      " |  setStandardization(self, value: bool) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`standardization`.\n",
      " |  \n",
      " |  setTol(self, value: float) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`tol`.\n",
      " |  \n",
      " |  setWeightCol(self, value: str) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`weightCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any]}\n",
      " |  \n",
      " |  __orig_bases__ = (pyspark.ml.regression._JavaRegressor[ForwardRef('Lin...\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Predictor:\n",
      " |  \n",
      " |  setFeaturesCol(self: ~P, value: str) -> ~P\n",
      " |      Sets the value of :py:attr:`featuresCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setLabelCol(self: ~P, value: str) -> ~P\n",
      " |      Sets the value of :py:attr:`labelCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setPredictionCol(self: ~P, value: str) -> ~P\n",
      " |      Sets the value of :py:attr:`predictionCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  clear(self, param: pyspark.ml.param.Param) -> None\n",
      " |      Clears a param from the param map if it has been explicitly set.\n",
      " |  \n",
      " |  copy(self: 'JP', extra: Optional[ForwardRef('ParamMap')] = None) -> 'JP'\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          Extra parameters to copy to the new instance\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`JavaParams`\n",
      " |          Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Estimator:\n",
      " |  \n",
      " |  fit(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), List[ForwardRef('ParamMap')], Tuple[ForwardRef('ParamMap')], NoneType] = None) -> Union[~M, List[~M]]\n",
      " |      Fits a model to the input dataset with optional parameters.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
      " |          input dataset.\n",
      " |      params : dict or list or tuple, optional\n",
      " |          an optional param map that overrides embedded params. If a list/tuple of\n",
      " |          param maps is given, this calls fit on each param map and returns a list of\n",
      " |          models.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n",
      " |          fitted model(s)\n",
      " |  \n",
      " |  fitMultiple(self, dataset: pyspark.sql.dataframe.DataFrame, paramMaps: Sequence[ForwardRef('ParamMap')]) -> Iterator[Tuple[int, ~M]]\n",
      " |      Fits a model to the input dataset for each param map in `paramMaps`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
      " |          input dataset.\n",
      " |      paramMaps : :py:class:`collections.abc.Sequence`\n",
      " |          A Sequence of param maps.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`_FitMultipleIterator`\n",
      " |          A thread safe iterable which contains one model for each param map. Each\n",
      " |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n",
      " |          using `paramMaps[index]`. `index` values may not be sequential.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _LinearRegressionParams:\n",
      " |  \n",
      " |  getEpsilon(self) -> float\n",
      " |      Gets the value of epsilon or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _LinearRegressionParams:\n",
      " |  \n",
      " |  epsilon = Param(parent='undefined', name='epsilon', doc='T...s. Must b...\n",
      " |  \n",
      " |  loss = Param(parent='undefined', name='loss', doc='The ...imized. Supp...\n",
      " |  \n",
      " |  solver = Param(parent='undefined', name='solver', doc='Th...ation. Sup...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  getLabelCol(self) -> str\n",
      " |      Gets the value of labelCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  getFeaturesCol(self) -> str\n",
      " |      Gets the value of featuresCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  getPredictionCol(self) -> str\n",
      " |      Gets the value of predictionCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasRegParam:\n",
      " |  \n",
      " |  getRegParam(self) -> float\n",
      " |      Gets the value of regParam or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasRegParam:\n",
      " |  \n",
      " |  regParam = Param(parent='undefined', name='regParam', doc='regularizat...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasElasticNetParam:\n",
      " |  \n",
      " |  getElasticNetParam(self) -> float\n",
      " |      Gets the value of elasticNetParam or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasElasticNetParam:\n",
      " |  \n",
      " |  elasticNetParam = Param(parent='undefined', name='elasticNetParam'...L...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n",
      " |  \n",
      " |  getMaxIter(self) -> int\n",
      " |      Gets the value of maxIter or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n",
      " |  \n",
      " |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasTol:\n",
      " |  \n",
      " |  getTol(self) -> float\n",
      " |      Gets the value of tol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasTol:\n",
      " |  \n",
      " |  tol = Param(parent='undefined', name='tol', doc='the c...ence toleranc...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasFitIntercept:\n",
      " |  \n",
      " |  getFitIntercept(self) -> bool\n",
      " |      Gets the value of fitIntercept or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasFitIntercept:\n",
      " |  \n",
      " |  fitIntercept = Param(parent='undefined', name='fitIntercept', doc='whe...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasStandardization:\n",
      " |  \n",
      " |  getStandardization(self) -> bool\n",
      " |      Gets the value of standardization or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasStandardization:\n",
      " |  \n",
      " |  standardization = Param(parent='undefined', name='standardization'...t...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  getWeightCol(self) -> str\n",
      " |      Gets the value of weightCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasSolver:\n",
      " |  \n",
      " |  getSolver(self) -> str\n",
      " |      Gets the value of solver or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasAggregationDepth:\n",
      " |  \n",
      " |  getAggregationDepth(self) -> int\n",
      " |      Gets the value of aggregationDepth or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasAggregationDepth:\n",
      " |  \n",
      " |  aggregationDepth = Param(parent='undefined', name='aggregationDepth', ...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasLoss:\n",
      " |  \n",
      " |  getLoss(self) -> str\n",
      " |      Gets the value of loss or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasMaxBlockSizeInMB:\n",
      " |  \n",
      " |  getMaxBlockSizeInMB(self) -> float\n",
      " |      Gets the value of maxBlockSizeInMB or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxBlockSizeInMB:\n",
      " |  \n",
      " |  maxBlockSizeInMB = Param(parent='undefined', name='maxBlockSizeInMB......\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self) -> str\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra: Optional[ForwardRef('ParamMap')] = None) -> 'ParamMap'\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          extra param values\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName: str) -> bool\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self) -> pyspark.ml.util.JavaMLWriter\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path: str) -> None\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path: str) -> ~RL from abc.ABCMeta\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from abc.ABCMeta\n",
      " |      Parameterizes a generic class.\n",
      " |      \n",
      " |      At least, parameterizing a generic class is the *main* thing this method\n",
      " |      does. For example, for some generic class `Foo`, this is called when we\n",
      " |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |      \n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "|carat|cut|color|clarity|depth|table|price|  x|  y|  z|\n",
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "|    0|  0|    0|      0|    0|    0|    0|  0|  0|  0|\n",
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Como vamos a predecir 'price', borramos filas donde 'price' sea nan:\n",
    "df_regresion = df.dropna(subset=['price'])\n",
    "\n",
    "# contar nulos en todas las columnas: equivalente a pandas df.isna().sum()\n",
    "df_regresion.select([sum(col(c).isNull().cast('int')).alias(c) for c in df_regresion.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|carat|    cut|color|clarity|depth|table|label|   x|   y|   z|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "| 0.23|  Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n",
      "| 0.21|Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n",
      "| 0.23|   Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renombrar la columna 'price' a 'label' para usarla como la columna objetivo\n",
    "df_regresion = df_regresion.withColumnRenamed('price', 'label') #.select('features', 'label')\n",
    "df_regresion.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seleccionar los nombres de las columnas a las que vamos a aplicar Preprocesados\n",
    "# Identificar columnas numéricas y categóricas\n",
    "numerical_cols = [field.name for field in df_regresion.schema.fields if isinstance(field.dataType, NumericType) and field.name != 'label']\n",
    "categorical_cols = [field.name for field in df_regresion.schema.fields if isinstance(field.dataType, StringType)]\n",
    "label_col = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carat', 'depth', 'table', 'x', 'y', 'z']\n",
      "['cut', 'color', 'clarity']\n"
     ]
    }
   ],
   "source": [
    "print(numerical_cols)\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cut_indexed', 'color_indexed', 'clarity_indexed']\n"
     ]
    }
   ],
   "source": [
    "# Crear indexadores para las columnas categóricas\n",
    "# Indexers para las features de la entrada que no son la columna label a predecir\n",
    "# crea un objeto StringIndexer por cada columna categórica a indexar\n",
    "indexers_features = [\n",
    "    StringIndexer(inputCol=c, outputCol=c + '_indexed', handleInvalid='keep') for c in categorical_cols\n",
    "]\n",
    "categorical_cols_indexed = [c + '_indexed' for c in categorical_cols]\n",
    "print(categorical_cols_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cut_indexed_imputed', 'color_indexed_imputed', 'clarity_indexed_imputed']\n"
     ]
    }
   ],
   "source": [
    "# Imputar valores faltantes en las columnas categóricas indexadas usando la moda\n",
    "imputer_categorical = Imputer(\n",
    "    inputCols=categorical_cols_indexed,\n",
    "    outputCols=[c + '_imputed' for c in categorical_cols_indexed],\n",
    "    strategy='mode'\n",
    ")\n",
    "categorical_cols_indexed_imputed = [c + '_imputed' for c in categorical_cols_indexed]\n",
    "print(categorical_cols_indexed_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cut_indexed_imputed_onehot', 'color_indexed_imputed_onehot', 'clarity_indexed_imputed_onehot']\n"
     ]
    }
   ],
   "source": [
    "# Crear codificadores OneHot para las columnas categóricas imputadas\n",
    "encoders_onehot = [\n",
    "    OneHotEncoder(inputCol=c, outputCol=c + '_onehot') \n",
    "    for c in categorical_cols_indexed_imputed\n",
    "]\n",
    "categorical_cols_onehot = [c + '_onehot' for c in categorical_cols_indexed_imputed]\n",
    "print(categorical_cols_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carat_imputed', 'depth_imputed', 'table_imputed', 'x_imputed', 'y_imputed', 'z_imputed']\n"
     ]
    }
   ],
   "source": [
    "# Imputar valores faltantes en las columnas numéricas usando la mediana\n",
    "imputer_numerical = Imputer(\n",
    "    inputCols=numerical_cols,\n",
    "    outputCols=[c + '_imputed' for c in numerical_cols],\n",
    "    strategy='median'\n",
    ")\n",
    "numerical_cols_imputed = [c + '_imputed' for c in numerical_cols]\n",
    "print(numerical_cols_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Opcional) escalar numéricas con MinMaxScaler\n",
    "# Ensamblar las columnas numéricas imputadas en un vector y escalar los valores\n",
    "assembler_numerical = VectorAssembler(\n",
    "    inputCols=numerical_cols_imputed,\n",
    "    outputCol='numeric_features'\n",
    ")\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol='numeric_features',\n",
    "    outputCol='numeric_features_scaled'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['numeric_features_scaled', 'cut_indexed_imputed_onehot', 'color_indexed_imputed_onehot', 'clarity_indexed_imputed_onehot']\n"
     ]
    }
   ],
   "source": [
    "# Ensamblar todas las características (numéricas escaladas + categóricas codificadas) en una sola columna 'features'\n",
    "all_columns = ['numeric_features_scaled'] + categorical_cols_onehot\n",
    "print(all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensamblar todo: numéricas + categóricas y obtener 'features'\n",
    "assembler_all = VectorAssembler(\n",
    "    inputCols=all_columns,\n",
    "    outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un modelo de Random Forest Regressor\n",
    "# regressor = RandomForestRegressor(seed=42, labelCol='price')\n",
    "regressor = RandomForestRegressor(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "# particionamiento de datos antes del pipeline\n",
    "df_train, df_test = df_regresion.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el pipeline con todas las etapas de preprocesamiento y el modelo\n",
    "pipeline = Pipeline(stages = [\n",
    "    # 2. Indexers para columnas categóricas: 'cut' 'color' y 'clarity'\n",
    "    *indexers_features, # ponemos * porque es una lista de objetos\n",
    "    # 3. Imputer para categóricas\n",
    "    imputer_categorical,\n",
    "    # 4. One Hot Encoders para categóricas\n",
    "    *encoders_onehot, # ponemos * porque es una lista de objetos\n",
    "    # 5. Imputer para numéricas: 'carat', 'depth', 'table', 'x', 'y' y 'z'\n",
    "    imputer_numerical,\n",
    "    # 6. Ensamblar numéricas + escalado\n",
    "    assembler_numerical,\n",
    "    scaler,\n",
    "    # 7. Ensamblar numéricas escaladas + categóricas en una sola columna 'features'\n",
    "    assembler_all,\n",
    "    # 8. modelo\n",
    "    regressor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "# aplicamos el modelo a df_train y df_test, haciendo el fit en df_train y el transform en df_test\n",
    "pipeline_model = pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+-----------+-------------+---------------+-------------------+---------------------+-----------------------+--------------------------+----------------------------+------------------------------+-------------+-------------+-------------+---------+---------+---------+--------------------+-----------------------+--------------------+-----------------+\n",
      "|carat|    cut|color|clarity|depth|table|label|   x|   y|   z|cut_indexed|color_indexed|clarity_indexed|cut_indexed_imputed|color_indexed_imputed|clarity_indexed_imputed|cut_indexed_imputed_onehot|color_indexed_imputed_onehot|clarity_indexed_imputed_onehot|carat_imputed|depth_imputed|table_imputed|x_imputed|y_imputed|z_imputed|    numeric_features|numeric_features_scaled|            features|       prediction|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+-----------+-------------+---------------+-------------------+---------------------+-----------------------+--------------------------+----------------------------+------------------------------+-------------+-------------+-------------+---------+---------+---------+--------------------+-----------------------+--------------------+-----------------+\n",
      "|  0.2|  Ideal|    E|    VS2| 62.2| 57.0|  367|3.76|3.73|2.33|        0.0|          1.0|            1.0|                0.0|                  1.0|                    1.0|             (4,[0],[1.0])|               (6,[1],[1.0])|                 (7,[1],[1.0])|          0.2|         62.2|         57.0|     3.76|     3.73|     2.33|[0.20000000298023...|   [0.0,0.5333333545...|(23,[1,2,3,4,5,6,...|696.6891922788168|\n",
      "|  0.2|Premium|    E|    VS2| 59.0| 60.0|  367|3.81|3.78|2.24|        1.0|          1.0|            1.0|                1.0|                  1.0|                    1.0|             (4,[1],[1.0])|               (6,[1],[1.0])|                 (7,[1],[1.0])|          0.2|         59.0|         60.0|     3.81|     3.78|     2.24|[0.20000000298023...|   [0.0,0.4444444444...|(23,[1,2,3,4,5,7,...| 691.788403137286|\n",
      "|  0.2|Premium|    E|    VS2| 59.8| 62.0|  367|3.79|3.77|2.26|        1.0|          1.0|            1.0|                1.0|                  1.0|                    1.0|             (4,[1],[1.0])|               (6,[1],[1.0])|                 (7,[1],[1.0])|          0.2|         59.8|         62.0|     3.79|     3.77|     2.26|[0.20000000298023...|   [0.0,0.4666666454...|(23,[1,2,3,4,5,7,...| 691.788403137286|\n",
      "| 0.21|Premium|    D|    VS2| 59.1| 62.0|  386|3.89|3.86|2.29|        1.0|          4.0|            1.0|                1.0|                  4.0|                    1.0|             (4,[1],[1.0])|               (6,[4],[1.0])|                 (7,[1],[1.0])|         0.21|         59.1|         62.0|     3.89|     3.86|     2.29|[0.20999999344348...|   [0.00207899999867...|(23,[0,1,2,3,4,5,...| 698.419859083636|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+-----------+-------------+---------------+-------------------+---------------------+-----------------------+--------------------------+----------------------------+------------------------------+-------------+-------------+-------------+---------+---------+---------+--------------------+-----------------------+--------------------+-----------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Realizar predicciones en el conjunto de prueba\n",
    "df_pred = pipeline_model.transform(df_test)\n",
    "df_pred.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 0.9070336070647529\n",
      "mae 684.109865656974\n",
      "mse 1512592.3852326302\n",
      "rmse 1229.8749469895833\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo usando diferentes métricas de regresión\n",
    "evaluator_r2 = RegressionEvaluator(metricName='r2')\n",
    "evaluator_mae = RegressionEvaluator(metricName='mae')\n",
    "evaluator_mse = RegressionEvaluator(metricName='mse')\n",
    "evaluator_rmse = RegressionEvaluator(metricName='rmse')\n",
    "\n",
    "print('r2', evaluator_r2.evaluate(df_pred))\n",
    "print('mae', evaluator_mae.evaluate(df_pred))\n",
    "print('mse', evaluator_mse.evaluate(df_pred))\n",
    "print('rmse', evaluator_rmse.evaluate(df_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch y CrossValidation para optimizar el modelo\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(regressor.numTrees, [5, 10, 15, 20, 25, 30]) # por defecto es 20\n",
    "    .addGrid(regressor.maxDepth, [3, 5, 10, 15]) # por defecto es 5 (rango de 0 a 30)\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación cruzada para precio\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid, #parametros para grid search hyper parameter tuning\n",
    "    evaluator=evaluator_r2,\n",
    "    numFolds=3, # 3 por defecto\n",
    "    parallelism=4, \n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo con CrossValidation\n",
    "cv_model = crossval.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar predicciones con el mejor modelo encontrado\n",
    "df_pred = cv_model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 0.9661739441819888\n",
      "mae 378.4620268242738\n",
      "mse 550360.5425286874\n",
      "rmse 741.8628866095725\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el mejor modelo\n",
    "print('r2', evaluator_r2.evaluate(df_pred))\n",
    "print('mae', evaluator_mae.evaluate(df_pred))\n",
    "print('mse', evaluator_mse.evaluate(df_pred))\n",
    "print('rmse', evaluator_rmse.evaluate(df_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='RandomForestRegressor_53afc85602a1', name='bootstrap', doc='Whether bootstrap samples are used when building trees.'): True, Param(parent='RandomForestRegressor_53afc85602a1', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False, Param(parent='RandomForestRegressor_53afc85602a1', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10, Param(parent='RandomForestRegressor_53afc85602a1', name='featureSubsetStrategy', doc=\"The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto'\"): 'auto', Param(parent='RandomForestRegressor_53afc85602a1', name='featuresCol', doc='features column name.'): 'features', Param(parent='RandomForestRegressor_53afc85602a1', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance'): 'variance', Param(parent='RandomForestRegressor_53afc85602a1', name='labelCol', doc='label column name.'): 'label', Param(parent='RandomForestRegressor_53afc85602a1', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '', Param(parent='RandomForestRegressor_53afc85602a1', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32, Param(parent='RandomForestRegressor_53afc85602a1', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 15, Param(parent='RandomForestRegressor_53afc85602a1', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256, Param(parent='RandomForestRegressor_53afc85602a1', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, Param(parent='RandomForestRegressor_53afc85602a1', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1, Param(parent='RandomForestRegressor_53afc85602a1', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0, Param(parent='RandomForestRegressor_53afc85602a1', name='numTrees', doc='Number of trees to train (>= 1).'): 30, Param(parent='RandomForestRegressor_53afc85602a1', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='RandomForestRegressor_53afc85602a1', name='seed', doc='random seed.'): 42, Param(parent='RandomForestRegressor_53afc85602a1', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}\n",
      "30\n",
      "15\n",
      "(23,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],[0.24482164358791156,0.0071918569163432105,0.004924525943113082,0.19582134015306527,0.2510326955608884,0.21105444355745062,0.0013617433189972725,0.0006203401437602568,0.000724163689972199,0.0007434571229592134,0.005220996173825739,0.004867917480515206,0.005176504403918618,0.002931956054873359,0.005551273100985277,0.0037614820558610415,0.007871303683818356,0.004754076445680741,0.016439628603495245,0.004691613178238502,0.008214939156874292,0.0061763754415468846,0.00604572422590581])\n"
     ]
    }
   ],
   "source": [
    "# Obtener los mejores parámetros del modelo\n",
    "best_model = cv_model.bestModel\n",
    "best_rf =best_model.stages[-1]\n",
    "print(best_rf.extractParamMap())\n",
    "print(best_rf.getNumTrees)\n",
    "print(best_rf.getOrDefault('maxDepth'))\n",
    "print(best_rf.featureImportances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "|carat|cut|color|clarity|depth|table|price|  x|  y|  z|\n",
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "|    0|  0|    0|      0|    0|    0|    0|  0|  0|  0|\n",
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eliminar filas con valores nulos en la columna 'cut'\n",
    "# Como vamos a predecir 'cut' borramos filas donde 'cut' sea nan:\n",
    "df_class = df.dropna(subset=['cut'])\n",
    "\n",
    "# contar nulos en todas las columnas: equivalente a pandas df.isna().sum()\n",
    "df_class.select([sum(col(c).isNull().cast('int')).alias(c) for c in df_class.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, asignamos como label a la variable cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar columnas numéricas y categóricas para la clasificación\n",
    "# seleccionar los nombres de las columnas a las que aplicar Preprocesados\n",
    "numerical_cols = [field.name for field in df_class.schema.fields if isinstance(field.dataType, NumericType)]\n",
    "categorical_cols = [field.name for field in df_class.schema.fields if isinstance(field.dataType, StringType) and field.name != 'cut']\n",
    "label_col = 'cut'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un indexador para la columna objetivo 'cut'\n",
    "# Indexer para 'cut' la columna a predecir\n",
    "indexer_label = StringIndexer(\n",
    "    inputCol=label_col, # va a indexar cut que es la que queremos predecir\n",
    "    outputCol='label',\n",
    "    handleInvalid='keep'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color_indexed', 'clarity_indexed']\n"
     ]
    }
   ],
   "source": [
    "# Crear indexadores para las columnas categóricas de entrada\n",
    "# Indexers para las features de la entrada que no son la columna label a predecir\n",
    "# crea un objeto StringIndexer por cada columna categórica a indexar\n",
    "indexers_features = [\n",
    "    StringIndexer(inputCol=c, outputCol=c + '_indexed', handleInvalid='keep') for c in categorical_cols\n",
    "]\n",
    "categorical_cols_indexed = [c + '_indexed' for c in categorical_cols]\n",
    "print(categorical_cols_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color_indexed_imputed', 'clarity_indexed_imputed']\n"
     ]
    }
   ],
   "source": [
    "# Imputar valores faltantes en las columnas categóricas indexadas usando la moda\n",
    "imputer_categorical = Imputer(\n",
    "    inputCols=categorical_cols_indexed,\n",
    "    outputCols=[c + '_imputed' for c in categorical_cols_indexed],\n",
    "    strategy='mode'\n",
    ")\n",
    "categorical_cols_indexed_imputed = [c + '_imputed' for c in categorical_cols_indexed]\n",
    "print(categorical_cols_indexed_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color_indexed_imputed_onehot', 'clarity_indexed_imputed_onehot']\n"
     ]
    }
   ],
   "source": [
    "# Crear codificadores OneHot para las columnas categóricas imputadas\n",
    "encoders_onehot = [\n",
    "    OneHotEncoder(inputCol=c, outputCol=c + '_onehot') \n",
    "    for c in categorical_cols_indexed_imputed\n",
    "]\n",
    "categorical_cols_onehot = [c + '_onehot' for c in categorical_cols_indexed_imputed]\n",
    "print(categorical_cols_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carat_imputed', 'depth_imputed', 'table_imputed', 'price_imputed', 'x_imputed', 'y_imputed', 'z_imputed']\n"
     ]
    }
   ],
   "source": [
    "# Imputar valores faltantes en las columnas numéricas usando la mediana\n",
    "imputer_numerical = Imputer(\n",
    "    inputCols=numerical_cols,\n",
    "    outputCols=[c + '_imputed' for c in numerical_cols],\n",
    "    strategy='median'\n",
    ")\n",
    "numerical_cols_imputed = [c + '_imputed' for c in numerical_cols]\n",
    "print(numerical_cols_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensamblar las columnas numéricas imputadas en un vector y escalar los valores\n",
    "# (Opcional) escalar numéricas con MinMaxScaler\n",
    "assembler_numerical = VectorAssembler(\n",
    "    inputCols=numerical_cols_imputed,\n",
    "    outputCol='numeric_features'\n",
    ")\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol='numeric_features',\n",
    "    outputCol='numeric_features_scaled'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['numeric_features_scaled', 'color_indexed_imputed_onehot', 'clarity_indexed_imputed_onehot']\n"
     ]
    }
   ],
   "source": [
    "# Ensamblar todas las características (numéricas escaladas + categóricas codificadas) en una sola columna 'features'\n",
    "all_columns = ['numeric_features_scaled'] + categorical_cols_onehot\n",
    "print(all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensamblar todo: numéricas + categóricas y obtener features\n",
    "assembler_all = VectorAssembler(\n",
    "    inputCols=all_columns,\n",
    "    outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un modelo de Random Forest Classifier\n",
    "classifier = RandomForestClassifier(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "# particionamiento de datos\n",
    "df_train, df_test = df_class.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el pipeline con todas las etapas de preprocesamiento y el modelo de clasificación\n",
    "pipeline = Pipeline(stages = [\n",
    "    # 1. Indexer para columna categórica 'cut' StringIndexer porque es la columna a predecir\n",
    "    indexer_label,\n",
    "    # 2. Indexers para columnas categóricas: 'color' y 'clarity'\n",
    "    *indexers_features, # ponemos * porque es una lista de objetos\n",
    "    # 3. Imputer para categóricas\n",
    "    imputer_categorical,\n",
    "    # 4. One Hot Encoders para categóricas\n",
    "    *encoders_onehot, # ponemos * porque es una lista de objetos\n",
    "    # 5. Imputer para numéricas\n",
    "    imputer_numerical,\n",
    "    # 6. Ensamblar numéricas + escalado\n",
    "    assembler_numerical,\n",
    "    scaler,\n",
    "    # 7. Ensamblar numéricas escaladas + categóricas en una sola columna 'features'\n",
    "    assembler_all,\n",
    "    # 8. modelo de clasificación\n",
    "    classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "pipeline_model = pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar predicciones en el conjunto de prueba\n",
    "df_pred = pipeline_model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo de clasificación usando diferentes métricas\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(metricName='f1')\n",
    "evaluator_precision = MulticlassClassificationEvaluator(metricName='weightedPrecision')\n",
    "evaluator_recall = MulticlassClassificationEvaluator(metricName='weightedRecall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6690614350188818\n",
      "f1 0.6194954681688324\n",
      "precision 0.647494841273576\n",
      "recall 0.6690614350188818\n"
     ]
    }
   ],
   "source": [
    "print('accuracy', evaluator_accuracy.evaluate(df_pred))\n",
    "print('f1', evaluator_f1.evaluate(df_pred))\n",
    "print('precision', evaluator_precision.evaluate(df_pred))\n",
    "print('recall', evaluator_recall.evaluate(df_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch y validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch y CrossValidation para optimizar el modelo de clasificación\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(classifier.numTrees, [5, 10, 15, 20, 25, 30]) # por defecto es 20\n",
    "    .addGrid(classifier.maxDepth, [3, 5, 10, 15]) # por defecto es 5 rango de [0, 30]\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid, # Parámetros para grid search hyper parameter tuning\n",
    "    evaluator=evaluator_f1,\n",
    "    numFolds=3, # por defecto ya 3 folds\n",
    "    parallelism=4,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo con CrossValidation\n",
    "cv_model = crossval.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar predicciones con el mejor modelo encontrado\n",
    "df_pred = cv_model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6690614350188818\n",
      "f1 0.6194954681688324\n",
      "precision 0.647494841273576\n",
      "recall 0.6690614350188818\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el mejor modelo de clasificación\n",
    "print('accuracy', evaluator_accuracy.evaluate(df_pred))\n",
    "print('f1', evaluator_f1.evaluate(df_pred))\n",
    "print('precision', evaluator_precision.evaluate(df_pred))\n",
    "print('recall', evaluator_recall.evaluate(df_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='RandomForestRegressor_53afc85602a1', name='bootstrap', doc='Whether bootstrap samples are used when building trees.'): True, Param(parent='RandomForestRegressor_53afc85602a1', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False, Param(parent='RandomForestRegressor_53afc85602a1', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10, Param(parent='RandomForestRegressor_53afc85602a1', name='featureSubsetStrategy', doc=\"The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto'\"): 'auto', Param(parent='RandomForestRegressor_53afc85602a1', name='featuresCol', doc='features column name.'): 'features', Param(parent='RandomForestRegressor_53afc85602a1', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance'): 'variance', Param(parent='RandomForestRegressor_53afc85602a1', name='labelCol', doc='label column name.'): 'label', Param(parent='RandomForestRegressor_53afc85602a1', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '', Param(parent='RandomForestRegressor_53afc85602a1', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32, Param(parent='RandomForestRegressor_53afc85602a1', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 15, Param(parent='RandomForestRegressor_53afc85602a1', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256, Param(parent='RandomForestRegressor_53afc85602a1', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, Param(parent='RandomForestRegressor_53afc85602a1', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1, Param(parent='RandomForestRegressor_53afc85602a1', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0, Param(parent='RandomForestRegressor_53afc85602a1', name='numTrees', doc='Number of trees to train (>= 1).'): 30, Param(parent='RandomForestRegressor_53afc85602a1', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='RandomForestRegressor_53afc85602a1', name='seed', doc='random seed.'): 42, Param(parent='RandomForestRegressor_53afc85602a1', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}\n",
      "30\n",
      "15\n",
      "(23,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],[0.24482164358791156,0.0071918569163432105,0.004924525943113082,0.19582134015306527,0.2510326955608884,0.21105444355745062,0.0013617433189972725,0.0006203401437602568,0.000724163689972199,0.0007434571229592134,0.005220996173825739,0.004867917480515206,0.005176504403918618,0.002931956054873359,0.005551273100985277,0.0037614820558610415,0.007871303683818356,0.004754076445680741,0.016439628603495245,0.004691613178238502,0.008214939156874292,0.0061763754415468846,0.00604572422590581])\n"
     ]
    }
   ],
   "source": [
    "# Obtener los mejores parámetros del modelo de clasificación\n",
    "best_model = cv_model.bestModel\n",
    "best_rf =best_model.stages[-1]\n",
    "print(best_rf.extractParamMap())\n",
    "print(best_rf.getNumTrees)\n",
    "print(best_rf.getOrDefault('maxDepth'))\n",
    "print(best_rf.featureImportances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado\n",
    "pipeline_model.write().overwrite().save('pipeline_spark_diamonds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
